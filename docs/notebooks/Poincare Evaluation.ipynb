{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Poincare Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how well poincare embeddings trained using this [implementation](https://github.com/TatsuyaShirakawa/poincare-embedding) perform on the tasks detailed in the [original paper](https://arxiv.org/pdf/1705.08039.pdf).\n",
    "\n",
    "This is the list of tasks - \n",
    "1. WordNet reconstruction\n",
    "2. WordNet link prediction\n",
    "3. Link prediction in collaboration networks\n",
    "4. Lexical entailment on HyperLex\n",
    "\n",
    "A more detailed explanation of the tasks and the evaluation methodology is present in the individual evaluation subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Clone the [poincare-embedding](https://github.com/TatsuyaShirakawa/poincare-embedding) repository and follow the README to compile the sources into a binary. Set the variable below to the directory containing the `poincare-embedding` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to the path of the directory containing the poincare-embedding directory\n",
    "parent_directory = '/home/jayant/projects/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training\n",
    "\n",
    "### 2.1 Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# These directories are auto created in the current directory for storing poincare datasets and models\n",
    "data_directory = 'poincare_data'\n",
    "models_directory = os.path.join(data_directory, 'models')\n",
    "\n",
    "# Create directories\n",
    "! mkdir {data_directory}\n",
    "! mkdir {models_directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the WordNet data\n",
    "wordnet_file = os.path.join(data_directory, 'wordnet_noun_hypernyms.tsv')\n",
    "! python {parent_directory}/poincare-embedding/scripts/create_wordnet_noun_hierarchy.py {wordnet_data_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperlex_url = \"http://people.ds.cam.ac.uk/iv250/paper/hyperlex/hyperlex-data.zip\"\n",
    "! wget {hyperlex_url} -P {data_directory}\n",
    "! unzip {data_directory}/hyperlex-data.zip -d {data_directory}\n",
    "hyperlex_file = os.path.join(data_directory, 'nouns-verbs', 'hyperlex-nouns.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Traing C++ embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import check_output\n",
    "\n",
    "def train_cpp_model(binary_path, data_file, output_file, dim, epochs, neg, num_threads, seed=0):\n",
    "    \"\"\"Train a poincare embedding using the c++ implementation\n",
    "    \n",
    "    Args:\n",
    "        binary_path (str): Path to the compiled c++ implementation binary\n",
    "        \n",
    "    \"\"\"\n",
    "    args = {\n",
    "        'dim': dim,\n",
    "        'max_epoch': epochs,\n",
    "        'neg_size': neg,\n",
    "        'num_thread': num_threads,\n",
    "        'learning_rate_init': 0.1,\n",
    "        'learning_rate_final': 0.0001,\n",
    "    }\n",
    "    cmd = [binary_path, data_file, output_file]\n",
    "    for option, value in args.items():\n",
    "        cmd.append(\"--%s\" % option)\n",
    "        cmd.append(str(value))\n",
    "\n",
    "    return check_output(args=cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_binary_path = os.path.join(parent_directory, 'poincare-embedding', 'work', 'poincare_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sizes = [5, 10, 20, 50, 100]\n",
    "neg_sizes = [10, 20]\n",
    "epochs = [50, 100]\n",
    "threads = [8, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with size 5 neg 10 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_10_dim_5\n",
      "Training model with size 10 neg 10 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_10_dim_10\n",
      "Training model with size 20 neg 10 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_10_dim_20\n",
      "Training model with size 50 neg 10 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_10_dim_50\n",
      "Training model with size 100 neg 10 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_10_dim_100\n",
      "Training model with size 5 neg 20 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_20_dim_5\n",
      "Training model with size 10 neg 20 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_20_dim_10\n",
      "Training model with size 20 neg 20 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_20_dim_20\n",
      "Training model with size 50 neg 20 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_20_dim_50\n",
      "Training model with size 100 neg 20 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_20_dim_100\n",
      "Training model with size 5 neg 10 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_10_dim_5\n",
      "Training model with size 10 neg 10 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_10_dim_10\n",
      "Training model with size 20 neg 10 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_10_dim_20\n",
      "Training model with size 50 neg 10 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_10_dim_50\n",
      "Training model with size 100 neg 10 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_10_dim_100\n",
      "Training model with size 5 neg 20 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_20_dim_5\n",
      "Training model with size 10 neg 20 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_20_dim_10\n",
      "Training model with size 20 neg 20 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_20_dim_20\n",
      "Training model with size 50 neg 20 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_20_dim_50\n",
      "Training model with size 100 neg 20 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_20_dim_100\n",
      "Training model with size 5 neg 10 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_10_dim_5\n",
      "Training model with size 10 neg 10 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_10_dim_10\n",
      "Training model with size 20 neg 10 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_10_dim_20\n",
      "Training model with size 50 neg 10 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_10_dim_50\n",
      "Training model with size 100 neg 10 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_10_dim_100\n",
      "Training model with size 5 neg 20 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_20_dim_5\n",
      "Training model with size 10 neg 20 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_20_dim_10\n",
      "Training model with size 20 neg 20 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_20_dim_20\n",
      "Training model with size 50 neg 20 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_20_dim_50\n",
      "Training model with size 100 neg 20 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_20_dim_100\n",
      "Training model with size 5 neg 10 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_10_dim_5\n",
      "Training model with size 10 neg 10 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_10_dim_10\n",
      "Training model with size 20 neg 10 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_10_dim_20\n",
      "Training model with size 50 neg 10 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_10_dim_50\n",
      "Training model with size 100 neg 10 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_10_dim_100\n",
      "Training model with size 5 neg 20 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_20_dim_5\n",
      "Training model with size 10 neg 20 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_20_dim_10\n",
      "Training model with size 20 neg 20 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_20_dim_20\n",
      "Training model with size 50 neg 20 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_20_dim_50\n",
      "Training model with size 100 neg 20 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_20_dim_100\n"
     ]
    }
   ],
   "source": [
    "# Possibly re-write with permutations instead of nested loops?\n",
    "for epochs_ in epochs:\n",
    "    for threads_ in threads:\n",
    "        for neg_size in neg_sizes:\n",
    "            model_name = 'cpp_epochs_%d_threads_%d_neg_%d' % (epochs_, threads_, neg_size)\n",
    "            model_files[model_name] = {}\n",
    "            for model_size in model_sizes:\n",
    "                output_file_name = '%s_dim_%d' % (model_name, model_size)\n",
    "                output_file = os.path.join(models_directory, output_file_name)\n",
    "                print('Training model with size %d neg %d threads %d epochs %d, saving to %s' %\n",
    "                     (model_size, neg_size, threads_, epochs_, output_file))\n",
    "                out = train_cpp_model(\n",
    "                    cpp_binary_path, wordnet_data_file, output_file,\n",
    "                    model_size, epochs_, neg_size, threads_, seed=0)\n",
    "                model_files[model_name][model_size] = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 C++ embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from pygtrie import Trie\n",
    "from scipy.spatial.distance import euclidean, pdist\n",
    "from smart_open import smart_open\n",
    "\n",
    "def transform_cpp_embedding_to_kv(input_file, output_file, encoding='utf8'):\n",
    "    \"\"\"Given a C++ embedding tsv filepath, converts it to a KeyedVector-supported file\"\"\"\n",
    "    with smart_open(input_file, 'rb') as f:\n",
    "        lines = [line.decode(encoding) for line in f]\n",
    "    if not len(lines):\n",
    "         raise ValueError(\"file is empty\")\n",
    "    first_line = lines[0]\n",
    "    parts = first_line.rstrip().split(\"\\t\")\n",
    "    model_size = len(parts) - 1\n",
    "    vocab_size = len(lines)\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('%d %d\\n' % (vocab_size, model_size))\n",
    "        for line in lines:\n",
    "            f.write(line.replace('\\t', ' '))\n",
    "\n",
    "        \n",
    "class PoincareEmbedding(object):\n",
    "    \"\"\"Load and perform distance operations on poincare embedding\"\"\"\n",
    "\n",
    "    def __init__(self, keyed_vectors):\n",
    "        \"\"\"Initialize PoincareEmbedding via a KeyedVectors instance\"\"\"\n",
    "        self.kv = keyed_vectors\n",
    "        self.init_key_trie()\n",
    "        \n",
    "    def init_key_trie(self):\n",
    "        \"\"\"Setup trie containing vocab keys for quick prefix lookups\"\"\"\n",
    "        self.key_trie = Trie()\n",
    "        for key in self.kv.vocab:\n",
    "            self.key_trie[key] = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def poincare_dist(vector_1, vector_2):\n",
    "        \"\"\"Return poincare distance between two vectors\"\"\"\n",
    "        norm_1 = np.linalg.norm(vector_1)\n",
    "        norm_2 = np.linalg.norm(vector_2)\n",
    "        euclidean_dist = euclidean(vector_1, vector_2)\n",
    "        return np.arccosh(\n",
    "            1 + 2 * (\n",
    "                (euclidean_dist ** 2) / ((1 - norm_1 ** 2) * (1 - norm_2 ** 2))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    @classmethod\n",
    "    def load_poincare_cpp(cls, input_filename):\n",
    "        \"\"\"Load embedding trained via C++ Poincare model\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to tsv file containing embedding\n",
    "\n",
    "        Returns:\n",
    "            PoincareEmbedding instance\n",
    "\n",
    "        \"\"\"\n",
    "        keyed_vectors_filename = input_filename + '.kv'\n",
    "        transform_cpp_embedding_to_kv(input_filename, keyed_vectors_filename)\n",
    "        keyed_vectors = KeyedVectors.load_word2vec_format(keyed_vectors_filename)\n",
    "        os.unlink(keyed_vectors_filename)\n",
    "        return cls(keyed_vectors)\n",
    "\n",
    "    @classmethod\n",
    "    def load_poincare_numpy(cls, input_filename):\n",
    "        \"\"\"Load embedding trained via Python numpy Poincare model\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to pkl file containing embedding\n",
    "\n",
    "        Returns:\n",
    "            PoincareEmbedding instance\n",
    "\n",
    "        \"\"\"\n",
    "        keyed_vectors_filename = input_filename + '.kv'\n",
    "        transform_numpy_embedding_to_kv(input_filename, keyed_vectors_filename)\n",
    "        keyed_vectors = KeyedVectors.load_word2vec_format(keyed_vectors_filename)\n",
    "        os.unlink(keyed_vectors_filename)\n",
    "        return cls(keyed_vectors)\n",
    "    \n",
    "    def find_matching_keys(self, word):\n",
    "        \"\"\"Find all senses of given word in embedding vocabulary\"\"\"\n",
    "        matches = self.key_trie.items('%s.' % word)\n",
    "        matching_keys = [''.join(key_chars) for key_chars, value in matches]\n",
    "        return matching_keys\n",
    "\n",
    "    def get_vector(self, term):\n",
    "        \"\"\"Return vector for given term\"\"\"\n",
    "        return self.kv.word_vec(term)\n",
    "        \n",
    "    def get_all_distances(self, term):\n",
    "        \"\"\"Return distances to all terms for given term, including itself\"\"\"\n",
    "        term_vector = self.kv.word_vec(term)\n",
    "        all_vectors = self.kv.syn0\n",
    "        \n",
    "        euclidean_dists = np.linalg.norm(term_vector - all_vectors, axis=1)\n",
    "        norm = np.linalg.norm(term_vector)\n",
    "        all_norms = np.linalg.norm(all_vectors, axis=1)\n",
    "        return np.arccosh(\n",
    "            1 + 2 * (\n",
    "                (euclidean_dists ** 2) / ((1 - norm ** 2) * (1 - all_norms ** 2))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def get_distance(self, term_1, term_2):\n",
    "        \"\"\"Returns distance between vectors for input terms\n",
    "\n",
    "        Args:\n",
    "            term_1 (str)\n",
    "            term_2 (str)\n",
    "\n",
    "        Returns:\n",
    "            Poincare distance between the two terms (float)\n",
    "        \n",
    "        Note:\n",
    "            Raises KeyError if either term_1 or term_2 is absent from vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "        vector_1, vector_2 = self.kv[term_1], self.kv[term_2]\n",
    "        return self.poincare_dist(vector_1, vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, models in model_files.items():\n",
    "    embeddings[model_name] = {}\n",
    "    for model_size, model_file in models.items():\n",
    "        embeddings[model_name][model_size] = PoincareEmbedding.load_poincare_cpp(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Numpy embeddings\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def display_results(task_name, results):\n",
    "    \"\"\"Display evaluation results of multiple embeddings on a single task in a tabular format\n",
    "    \n",
    "    Args:\n",
    "        task_name (str): name the task being evaluated\n",
    "        results (dict): mapping between embeddings and corresponding results\n",
    "    \n",
    "    \"\"\"\n",
    "    header = PrettyTable()\n",
    "    # TODO: infer widths from table rather than hard-coding\n",
    "    header.field_names = [\" \" * 42, \" \" * 7 + \"Model Dimensions\" + \" \" * 8]\n",
    "\n",
    "    data = PrettyTable()\n",
    "    data.field_names = [\"Model Description\", \"Metric\"] + [str(dim) for dim in sorted(model_sizes)]\n",
    "    for model_name, model_results in results.items():\n",
    "        metrics = [metric for metric in model_results.keys()]\n",
    "        dims = sorted([dim for dim in model_results[metrics[0]].keys()])\n",
    "        row = [model_name, '\\n'.join(metrics)]\n",
    "        for dim in dims:\n",
    "            scores = ['%.2f' % model_results[metric][dim] for metric in metrics]\n",
    "            row.append('\\n'.join(scores))\n",
    "        data.add_row(row)\n",
    "    \n",
    "    header_lines = header.get_string(start=0, end=0).split(\"\\n\")[:2]\n",
    "    print('Results for %s task' % task_name)\n",
    "    print(\"\\n\".join(header_lines))\n",
    "    print(data)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 WordNet reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "\n",
    "class ReconstructionEvaluation(object):\n",
    "    \"\"\"Evaluating reconstruction on given network for given embedding\"\"\"\n",
    "    def __init__(self, filepath, embedding):\n",
    "        \"\"\"Initialize evaluation instance with tsv file containing relation pairs and embedding to be evaluated\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): path to tsv file containing relation pairs\n",
    "            embedding (PoincareEmbedding instance): embedding to be evaluated\n",
    "        \n",
    "        Returns\n",
    "            ReconstructionEvaluation instance\n",
    "\n",
    "        \"\"\"\n",
    "        items = set()\n",
    "        embedding_vocab = embedding.kv.vocab\n",
    "        relations = defaultdict(set)\n",
    "        with smart_open(filepath, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                assert len(row) == 2, 'Hypernym pair has more than two items'\n",
    "                item_1_index = embedding_vocab[row[0]].index\n",
    "                item_2_index = embedding_vocab[row[1]].index\n",
    "                relations[item_1_index].add(item_2_index)\n",
    "                items.update([item_1_index, item_2_index])\n",
    "        self.items = items\n",
    "        self.relations = relations\n",
    "        self.embedding = embedding\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_positive_relation_ranks(distances, positive_relations):\n",
    "        \"\"\"\n",
    "        Given a numpy array of all distances from an item and indices of its positive relations,\n",
    "        compute ranks of positive relations\n",
    "        \n",
    "        Args:\n",
    "            distances (numpy float array): np array of all distances for a specific item\n",
    "            positive_relations (list): list of indices of positive relations for the item\n",
    "        \n",
    "        Returns:\n",
    "            list of ranks of positive items in the same order as `positive_indices`\n",
    "        \"\"\"\n",
    "        positive_relation_distances = distances[positive_relations]\n",
    "        negative_relation_distances = np.ma.array(distances, mask=False)\n",
    "        negative_relation_distances.mask[positive_relations] = True\n",
    "        # Compute how many negative relation distances are less than each positive relation distance, plus 1 for rank\n",
    "        ranks = (negative_relation_distances < positive_relation_distances[:, np.newaxis]).sum(axis=1) + 1\n",
    "        return list(ranks) \n",
    "    \n",
    "    def evaluate_metric(self, metric, max_n=None):\n",
    "        \"\"\"Evaluate given metric for the reconstruction task\n",
    "            \n",
    "        Args:\n",
    "            metric (str): accepted values are 'mean_rank' and 'MAP'\n",
    "            max_n (int or None): Maximum number of positive relations to evaluate, all if max_n is None\n",
    "        \n",
    "        Returns:\n",
    "            Computed value of given metric (float)\n",
    "\n",
    "        \"\"\"\n",
    "        if metric == 'mean_rank':\n",
    "            return self.evaluate_mean_rank(max_n)\n",
    "        elif metric == 'MAP':\n",
    "            return self.evaluate_map(max_n)\n",
    "        else:\n",
    "            raise ValueError('Invalid value for metric')\n",
    "\n",
    "    def evaluate_mean_rank(self, max_n=None):\n",
    "        \"\"\"Evaluate mean rank and MAP for reconstruction\n",
    "            \n",
    "        Args:\n",
    "            max_n (int or None): Maximum number of positive relations to evaluate, all if max_n is None\n",
    "        \n",
    "        Returns:\n",
    "            Computed value of mean rank (float)\n",
    "\n",
    "        \"\"\"\n",
    "        ranks = []\n",
    "        for i, item in enumerate(self.items, start=1):\n",
    "            if item not in self.relations:\n",
    "                continue\n",
    "            item_relations = list(self.relations[item])\n",
    "            item_term = self.embedding.kv.index2word[item]\n",
    "            item_distances = self.embedding.get_all_distances(item_term)\n",
    "            positive_relation_ranks = self.get_positive_relation_ranks(item_distances, item_relations)\n",
    "            ranks += positive_relation_ranks\n",
    "            if max_n is not None and i > max_n:\n",
    "                break\n",
    "        return np.mean(ranks)\n",
    "    \n",
    "    def evaluate_map(self, max_n=None):\n",
    "        \"\"\"Evaluate MAP (Mean Average Precision) for reconstruction\n",
    "            \n",
    "        Args:\n",
    "            max_n (int or None): Maximum number of positive relations to evaluate, all if max_n is None\n",
    "        \n",
    "        Returns:\n",
    "            Computed value of MAP (float)\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_results = {}\n",
    "metrics = ['mean_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 20\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 10\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 100\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 50\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 5\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 20\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 10\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 100\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 50\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 5\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 20\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 10\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 100\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 50\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 5\n"
     ]
    }
   ],
   "source": [
    "for model_name, models in embeddings.items():\n",
    "    reconstruction_results[model_name] = {}\n",
    "    for metric in metrics:\n",
    "        reconstruction_results[model_name][metric] = {}\n",
    "    for model_size, embedding in models.items():\n",
    "        print('Evaluating model %s of size %d' % (model_name, model_size))\n",
    "        eval_instance = ReconstructionEvaluation(wordnet_file, embedding)\n",
    "        for metric in metrics:\n",
    "            reconstruction_results[model_name][metric][model_size] = eval_instance.evaluate_metric(metric, max_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for WordNet Reconstruction task\n",
      "+--------------------------------------------+---------------------------------+\n",
      "|                                            |        Model Dimensions         |\n",
      "+--------------------------------+-----------+--------+--------+-------+-------+-------+\n",
      "|       Model Description        |   Metric  |   5    |   10   |   20  |   50  |  100  |\n",
      "+--------------------------------+-----------+--------+--------+-------+-------+-------+\n",
      "| cpp_epochs_50_threads_8_neg_10 | mean_rank | 268.44 | 129.66 | 86.78 | 76.58 | 71.15 |\n",
      "| cpp_epochs_50_threads_8_neg_20 | mean_rank | 251.71 | 145.56 | 96.18 | 72.44 | 57.26 |\n",
      "| cpp_epochs_50_threads_1_neg_10 | mean_rank | 325.21 | 107.59 | 71.23 | 61.48 | 60.01 |\n",
      "+--------------------------------+-----------+--------+--------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "display_results('WordNet Reconstruction', reconstruction_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 WordNet link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_test_split(data_file, test_ratio=0.1):\n",
    "    \"\"\"Creates train and test files from given data file, returns train/test file names\n",
    "    \n",
    "    Args:\n",
    "        data_file (str): path to data file for which train/test split is to be created\n",
    "        test_ratio (float): fraction of lines to be used for test data\n",
    "    \n",
    "    Returns\n",
    "        (train_file, test_file): tuple of strings with train file and test file paths\n",
    "    \"\"\"\n",
    "    root_nodes, leaf_nodes = get_root_and_leaf_nodes(data_file)\n",
    "    test_line_candidates = []\n",
    "    line_count = 0\n",
    "    all_nodes = set()\n",
    "    with open(data_file, 'rb') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            node_1, node_2 = line.split()\n",
    "            all_nodes.update([node_1, node_2])\n",
    "            if (\n",
    "                    node_1 not in leaf_nodes\n",
    "                    and node_2 not in leaf_nodes\n",
    "                    and node_1 not in root_nodes\n",
    "                    and node_2 not in root_nodes\n",
    "                ):\n",
    "                test_line_candidates.append(i)\n",
    "            line_count += 1\n",
    "\n",
    "    num_test_lines = int(test_ratio * line_count)\n",
    "    if num_test_lines > len(test_line_candidates):\n",
    "        raise ValueError('Not enough candidate relations for test set')\n",
    "    print('Choosing %d test lines from %d candidates' % (num_test_lines, len(test_line_candidates)))\n",
    "    test_line_indices = set(random.sample(test_line_candidates, num_test_lines))\n",
    "    train_line_indices = set(l for l in range(line_count) if l not in test_line_indices)\n",
    "    \n",
    "    train_filename = data_file + '.train'\n",
    "    test_filename = data_file + '.test'\n",
    "    train_set_nodes = set()\n",
    "    with open(data_file, 'rb') as f:\n",
    "        train_file = open(train_filename, 'wb')\n",
    "        test_file = open(test_filename, 'wb')\n",
    "        for i, line in enumerate(f):\n",
    "            if i in train_line_indices:\n",
    "                train_set_nodes.update(line.split())\n",
    "                train_file.write(line)\n",
    "            elif i in test_line_indices:\n",
    "                test_file.write(line)\n",
    "            else:\n",
    "                raise AssertionError('Line %d not present in either train or test line indices' % i)\n",
    "        train_file.close()\n",
    "        test_file.close()\n",
    "    assert len(train_set_nodes) == len(all_nodes), 'Not all nodes from dataset present in train set relations'\n",
    "    return (train_filename, test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_and_leaf_nodes(data_file):\n",
    "    \"\"\"Return keys of root and leaf nodes from a file with transitive closure relations\n",
    "    \n",
    "    Args:\n",
    "        data_file(str): file path containing transitive closure relations\n",
    "    \n",
    "    Returns:\n",
    "        (root_nodes, leaf_nodes) - tuple containing keys of root and leaf nodes\n",
    "    \"\"\"\n",
    "    root_candidates = set()\n",
    "    leaf_candidates = set()\n",
    "    with open(data_file, 'rb') as f:\n",
    "        for line in f:\n",
    "            nodes = line.split()\n",
    "            root_candidates.update(nodes)\n",
    "            leaf_candidates.update(nodes)\n",
    "    \n",
    "    with open(data_file, 'rb') as f:\n",
    "        for line in f:\n",
    "            node_1, node_2 = line.split()\n",
    "            if node_1 == node_2:\n",
    "                continue\n",
    "            leaf_candidates.discard(node_1)\n",
    "            root_candidates.discard(node_2)\n",
    "    \n",
    "    return (leaf_candidates, root_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing 74324 test lines from 126730 candidates\n"
     ]
    }
   ],
   "source": [
    "wordnet_train_file, wordnet_test_file = train_test_split(wordnet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Training and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training models for link prediction\n",
    "lp_model_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with size 5 neg 10 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_10_dim_5\n",
      "Training model with size 10 neg 10 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_10_dim_10\n",
      "Training model with size 20 neg 10 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_10_dim_20\n",
      "Training model with size 50 neg 10 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_10_dim_50\n",
      "Training model with size 100 neg 10 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_10_dim_100\n",
      "Training model with size 5 neg 20 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_20_dim_5\n",
      "Training model with size 10 neg 20 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_20_dim_10\n",
      "Training model with size 20 neg 20 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_20_dim_20\n",
      "Training model with size 50 neg 20 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_20_dim_50\n",
      "Training model with size 100 neg 20 threads 8 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_8_neg_20_dim_100\n",
      "Training model with size 5 neg 10 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_10_dim_5\n",
      "Training model with size 10 neg 10 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_10_dim_10\n",
      "Training model with size 20 neg 10 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_10_dim_20\n",
      "Training model with size 50 neg 10 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_10_dim_50\n",
      "Training model with size 100 neg 10 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_10_dim_100\n",
      "Training model with size 5 neg 20 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_20_dim_5\n",
      "Training model with size 10 neg 20 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_20_dim_10\n",
      "Training model with size 20 neg 20 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_20_dim_20\n",
      "Training model with size 50 neg 20 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_20_dim_50\n",
      "Training model with size 100 neg 20 threads 1 epochs 50, saving to poincare_data/models/cpp_epochs_50_threads_1_neg_20_dim_100\n",
      "Training model with size 5 neg 10 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_10_dim_5\n",
      "Training model with size 10 neg 10 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_10_dim_10\n",
      "Training model with size 20 neg 10 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_10_dim_20\n",
      "Training model with size 50 neg 10 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_10_dim_50\n",
      "Training model with size 100 neg 10 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_10_dim_100\n",
      "Training model with size 5 neg 20 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_20_dim_5\n",
      "Training model with size 10 neg 20 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_20_dim_10\n",
      "Training model with size 20 neg 20 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_20_dim_20\n",
      "Training model with size 50 neg 20 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_20_dim_50\n",
      "Training model with size 100 neg 20 threads 8 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_8_neg_20_dim_100\n",
      "Training model with size 5 neg 10 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_10_dim_5\n",
      "Training model with size 10 neg 10 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_10_dim_10\n",
      "Training model with size 20 neg 10 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_10_dim_20\n",
      "Training model with size 50 neg 10 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_10_dim_50\n",
      "Training model with size 100 neg 10 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_10_dim_100\n",
      "Training model with size 5 neg 20 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_20_dim_5\n",
      "Training model with size 10 neg 20 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_20_dim_10\n",
      "Training model with size 20 neg 20 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_20_dim_20\n",
      "Training model with size 50 neg 20 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_20_dim_50\n",
      "Training model with size 100 neg 20 threads 1 epochs 100, saving to poincare_data/models/cpp_epochs_100_threads_1_neg_20_dim_100\n"
     ]
    }
   ],
   "source": [
    "# Possibly re-write with permutations instead of nested loops?\n",
    "for epochs_ in epochs:\n",
    "    for threads_ in threads:\n",
    "        for neg_size in neg_sizes:\n",
    "            model_name = 'cpp_epochs_%d_threads_%d_neg_%d' % (epochs_, threads_, neg_size)\n",
    "            lp_model_files[model_name] = {}\n",
    "            for model_size in model_sizes:\n",
    "                output_file_name = '%s_dim_%d' % (model_name, model_size)\n",
    "                output_file = os.path.join(models_directory, output_file_name)\n",
    "                print('Training model with size %d neg %d threads %d epochs %d, saving to %s' %\n",
    "                     (model_size, neg_size, threads_, epochs_, output_file))\n",
    "                out = train_cpp_model(\n",
    "                    cpp_binary_path, wordnet_train_file, output_file,\n",
    "                    model_size, epochs_, neg_size, threads_, seed=0)\n",
    "                lp_model_files[model_name][model_size] = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, models in lp_model_files.items():\n",
    "    lp_embeddings[model_name] = {}\n",
    "    for model_size, model_file in models.items():\n",
    "        lp_embeddings[model_name][model_size] = PoincareEmbedding.load_poincare_cpp(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictionEvaluation(object):\n",
    "    \"\"\"Evaluating reconstruction on given network for given embedding\"\"\"\n",
    "    def __init__(self, train_path, test_path, embedding):\n",
    "        \"\"\"Initialize evaluation instance with tsv file containing relation pairs and embedding to be evaluated\n",
    "        \n",
    "        Args:\n",
    "            train_path (str): path to tsv file containing relation pairs used for training\n",
    "            test_path (str): path to tsv file containing relation pairs to evaluate\n",
    "            embedding (PoincareEmbedding instance): embedding to be evaluated\n",
    "        \n",
    "        Returns\n",
    "            ReconstructionEvaluation instance\n",
    "\n",
    "        \"\"\"\n",
    "        items = set()\n",
    "        embedding_vocab = embedding.kv.vocab\n",
    "        relations = {'known': defaultdict(set), 'unknown': defaultdict(set)}\n",
    "        data_files = {'known': train_path, 'unknown': test_path}\n",
    "        for relation_type, data_file in data_files.items():\n",
    "            with smart_open(data_file, 'r') as f:\n",
    "                reader = csv.reader(f, delimiter='\\t')\n",
    "                for row in reader:\n",
    "                    assert len(row) == 2, 'Hypernym pair has more than two items'\n",
    "                    item_1_index = embedding_vocab[row[0]].index\n",
    "                    item_2_index = embedding_vocab[row[1]].index\n",
    "                    relations[relation_type][item_1_index].add(item_2_index)\n",
    "                    items.update([item_1_index, item_2_index])\n",
    "        self.items = items\n",
    "        self.relations = relations\n",
    "        self.embedding = embedding\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_unknown_relation_ranks(distances, unknown_relations, known_relations):\n",
    "        \"\"\"\n",
    "        Given a numpy array of distances and indices of known and unknown positive relations,\n",
    "        compute ranks of unknown positive relations\n",
    "        \n",
    "        Args:\n",
    "            distances (numpy float array): np array of all distances for a specific item\n",
    "            unknown_relations (list): list of indices of unknown positive relations\n",
    "            known_relations (list): list of indices of known positive relations\n",
    "            \n",
    "        Returns:\n",
    "            list of ranks of unknown relations in the same order as `unknown_relations`\n",
    "        \"\"\"\n",
    "        unknown_relation_distances = distances[unknown_relations]\n",
    "        negative_relation_distances = np.ma.array(distances, mask=False)\n",
    "        negative_relation_distances.mask[unknown_relations] = True\n",
    "        negative_relation_distances.mask[known_relations] = True\n",
    "        # Compute how many negative relation distances are less than each unknown relation distance, plus 1 for rank\n",
    "        ranks = (negative_relation_distances < unknown_relation_distances[:, np.newaxis]).sum(axis=1) + 1\n",
    "        return list(ranks) \n",
    "    \n",
    "    def evaluate_metric(self, metric, max_n=None):\n",
    "        \"\"\"Evaluate given metric for the reconstruction task\n",
    "            \n",
    "        Args:\n",
    "            metric (str): accepted values are 'mean_rank' and 'MAP'\n",
    "            max_n (int or None): Maximum number of positive relations to evaluate, all if max_n is None\n",
    "        \n",
    "        Returns:\n",
    "            Computed value of given metric (float)\n",
    "\n",
    "        \"\"\"\n",
    "        if metric == 'mean_rank':\n",
    "            return self.evaluate_mean_rank(max_n)\n",
    "        elif metric == 'MAP':\n",
    "            return self.evaluate_map(max_n)\n",
    "        else:\n",
    "            raise ValueError('Invalid value for metric')\n",
    "\n",
    "    def evaluate_mean_rank(self, max_n=None):\n",
    "        \"\"\"Evaluate mean rank and MAP for reconstruction\n",
    "            \n",
    "        Args:\n",
    "            max_n (int or None): Maximum number of positive relations to evaluate, all if max_n is None\n",
    "        \n",
    "        Returns:\n",
    "            Computed value of mean rank (float)\n",
    "\n",
    "        \"\"\"\n",
    "        ranks = []\n",
    "        for i, item in enumerate(self.items, start=1):\n",
    "            if item not in self.relations['unknown']:  # No positive relations to predict for this node\n",
    "                continue\n",
    "            unknown_relations = list(self.relations['unknown'][item])\n",
    "            known_relations = list(self.relations['known'][item])\n",
    "            item_term = self.embedding.kv.index2word[item]\n",
    "            item_distances = self.embedding.get_all_distances(item_term)\n",
    "            unknown_relation_ranks = self.get_unknown_relation_ranks(item_distances, unknown_relations, known_relations)\n",
    "            ranks += unknown_relation_ranks\n",
    "            if max_n is not None and i > max_n:\n",
    "                break\n",
    "        return np.mean(ranks)\n",
    "    \n",
    "    def evaluate_map(self, max_n=None):\n",
    "        \"\"\"Evaluate MAP (Mean Average Precision) for reconstruction\n",
    "            \n",
    "        Args:\n",
    "            max_n (int or None): Maximum number of positive relations to evaluate, all if max_n is None\n",
    "        \n",
    "        Returns:\n",
    "            Computed value of MAP (float)\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_results = {}\n",
    "metrics = ['mean_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 20\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 10\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 100\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 50\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 5\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_20 of size 20\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_20 of size 10\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_20 of size 100\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_20 of size 50\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_20 of size 5\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 20\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 10\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 100\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 50\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 5\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 20\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 10\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 100\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 50\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 5\n",
      "Evaluating model cpp_epochs_100_threads_8_neg_10 of size 20\n",
      "Evaluating model cpp_epochs_100_threads_8_neg_10 of size 10\n",
      "Evaluating model cpp_epochs_100_threads_8_neg_10 of size 100\n",
      "Evaluating model cpp_epochs_100_threads_8_neg_10 of size 50\n",
      "Evaluating model cpp_epochs_100_threads_8_neg_10 of size 5\n",
      "Evaluating model cpp_epochs_100_threads_1_neg_20 of size 20\n",
      "Evaluating model cpp_epochs_100_threads_1_neg_20 of size 10\n",
      "Evaluating model cpp_epochs_100_threads_1_neg_20 of size 100\n",
      "Evaluating model cpp_epochs_100_threads_1_neg_20 of size 50\n",
      "Evaluating model cpp_epochs_100_threads_1_neg_20 of size 5\n",
      "Evaluating model cpp_epochs_100_threads_8_neg_20 of size 20\n",
      "Evaluating model cpp_epochs_100_threads_8_neg_20 of size 10\n",
      "Evaluating model cpp_epochs_100_threads_8_neg_20 of size 100\n",
      "Evaluating model cpp_epochs_100_threads_8_neg_20 of size 50\n",
      "Evaluating model cpp_epochs_100_threads_8_neg_20 of size 5\n",
      "Evaluating model cpp_epochs_100_threads_1_neg_10 of size 20\n",
      "Evaluating model cpp_epochs_100_threads_1_neg_10 of size 10\n",
      "Evaluating model cpp_epochs_100_threads_1_neg_10 of size 100\n",
      "Evaluating model cpp_epochs_100_threads_1_neg_10 of size 50\n",
      "Evaluating model cpp_epochs_100_threads_1_neg_10 of size 5\n"
     ]
    }
   ],
   "source": [
    "for model_name, models in lp_embeddings.items():\n",
    "    lp_results[model_name] = {}\n",
    "    for metric in metrics:\n",
    "        lp_results[model_name][metric] = {}\n",
    "    for model_size, embedding in models.items():\n",
    "        print('Evaluating model %s of size %d' % (model_name, model_size))\n",
    "        eval_instance = LinkPredictionEvaluation(wordnet_train_file, wordnet_test_file, embedding)\n",
    "        for metric in metrics:\n",
    "            lp_results[model_name][metric][model_size] = eval_instance.evaluate_metric(metric, max_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for WordNet Link Prediction task\n",
      "+--------------------------------------------+---------------------------------+\n",
      "|                                            |        Model Dimensions         |\n",
      "+---------------------------------+-----------+--------+--------+-------+-------+-------+\n",
      "|        Model Description        |   Metric  |   5    |   10   |   20  |   50  |  100  |\n",
      "+---------------------------------+-----------+--------+--------+-------+-------+-------+\n",
      "|  cpp_epochs_50_threads_8_neg_20 | mean_rank | 197.63 | 85.96  | 63.21 | 51.94 | 58.72 |\n",
      "|  cpp_epochs_50_threads_1_neg_20 | mean_rank | 191.36 | 92.09  | 53.68 | 47.57 | 46.26 |\n",
      "|  cpp_epochs_50_threads_1_neg_10 | mean_rank | 232.96 | 87.99  | 62.41 | 46.84 | 47.23 |\n",
      "|  cpp_epochs_50_threads_8_neg_10 | mean_rank | 208.80 | 104.13 | 77.91 | 63.97 | 75.42 |\n",
      "| cpp_epochs_100_threads_8_neg_10 | mean_rank | 202.66 | 99.25  | 77.94 | 67.56 | 66.49 |\n",
      "| cpp_epochs_100_threads_1_neg_20 | mean_rank | 147.81 | 79.82  | 48.12 | 42.29 | 41.96 |\n",
      "| cpp_epochs_100_threads_8_neg_20 | mean_rank | 160.67 | 105.16 | 70.21 | 56.91 | 46.13 |\n",
      "| cpp_epochs_100_threads_1_neg_10 | mean_rank | 157.65 | 72.03  | 50.60 | 39.28 | 39.18 |\n",
      "+---------------------------------+-----------+--------+--------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "display_results('WordNet Link Prediction', lp_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 HyperLex Lexical Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "class LexicalEntailmentEvaluation(object):\n",
    "    \"\"\"Evaluating reconstruction on given network for any embedding\"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"Initialize evaluation instance with HyperLex text file containing relation pairs\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): path to HyperLex text file\n",
    "        \n",
    "        Returns\n",
    "            LexicalEntailmentEvaluation instance\n",
    "\n",
    "        \"\"\"\n",
    "        expected_scores = {}\n",
    "        with smart_open(filepath, 'r') as f:\n",
    "            reader = csv.DictReader(f, delimiter=' ')\n",
    "            for row in reader:\n",
    "                word_1, word_2 = row['WORD1'], row['WORD2']\n",
    "                expected_scores[(word_1, word_2)] = float(row['AVG_SCORE'])\n",
    "        self.scores = expected_scores\n",
    "        self.alpha = 1000\n",
    "    \n",
    "    def score_function(self, embedding, word_1, word_2):\n",
    "        \"\"\"Given an embedding and two terms, return the predicted score for them (extent to which term_1 is a type of term_2)\"\"\"\n",
    "        try:\n",
    "            word_1_terms = embedding.find_matching_keys(word_1)\n",
    "            word_2_terms = embedding.find_matching_keys(word_2)\n",
    "        except KeyError:\n",
    "            raise ValueError(\"No matching terms found for either %s or %s\" % (word_1, word_2))\n",
    "        min_distance = np.inf\n",
    "        min_term_1, min_term_2 = None, None\n",
    "        for term_1 in word_1_terms:\n",
    "            for term_2 in word_2_terms:\n",
    "                distance = embedding.get_distance(term_1, term_2)\n",
    "                if distance < min_distance:\n",
    "                    min_term_1, min_term_2 = term_1, term_2\n",
    "                    min_distance = distance\n",
    "        assert min_term_1 is not None and min_term_2 is not None\n",
    "        vector_1, vector_2 = embedding.get_vector(min_term_1), embedding.get_vector(min_term_2)\n",
    "        norm_1, norm_2 = np.linalg.norm(vector_1), np.linalg.norm(vector_2)\n",
    "        return -1 * (1 + self.alpha * (norm_2 - norm_1)) * distance\n",
    "        \n",
    "    def evaluate_spearman(self, embedding):\n",
    "        \"\"\"Evaluate spearman scores for lexical entailment for given embedding\n",
    "            \n",
    "        Args:\n",
    "            embedding (PoincareEmbedding instance): embedding for which evaluation is to be done\n",
    "        \n",
    "        Returns:\n",
    "            spearman correlation score (float)\n",
    "\n",
    "        \"\"\"\n",
    "        predicted_scores = []\n",
    "        expected_scores = []\n",
    "        skipped = 0\n",
    "        count = 0\n",
    "        for (word_1, word_2), expected_score in self.scores.items():\n",
    "            try:\n",
    "                predicted_score = self.score_function(embedding, word_1, word_2)\n",
    "            except ValueError:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            count += 1\n",
    "            predicted_scores.append(predicted_score)\n",
    "            expected_scores.append(expected_score)\n",
    "        print('Skipped pairs: %d out of %d' % (skipped, len(self.scores)))\n",
    "        spearman = spearmanr(expected_scores, predicted_scores)\n",
    "        return spearman.correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment_results = {}\n",
    "eval_instance = LexicalEntailmentEvaluation(hyperlex_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 20\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 10\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 100\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 50\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_10 of size 5\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 20\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 10\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 100\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 50\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_8_neg_20 of size 5\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 20\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 10\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 100\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 50\n",
      "Skipped pairs: 182 out of 2163\n",
      "Evaluating model cpp_epochs_50_threads_1_neg_10 of size 5\n",
      "Skipped pairs: 182 out of 2163\n"
     ]
    }
   ],
   "source": [
    "for model_name, models in embeddings.items():\n",
    "    entailment_results[model_name] = {}\n",
    "    entailment_results[model_name]['spearman'] = {}\n",
    "    for model_size, embedding in models.items():\n",
    "        print('Evaluating model %s of size %d' % (model_name, model_size))\n",
    "        entailment_results[model_name]['spearman'][model_size] = eval_instance.evaluate_spearman(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Lexical Entailment (HyperLex) task\n",
      "+--------------------------------------------+---------------------------------+\n",
      "|                                            |        Model Dimensions         |\n",
      "+--------------------------------+----------+------+------+------+------+------+\n",
      "|       Model Description        |  Metric  |  5   |  10  |  20  |  50  | 100  |\n",
      "+--------------------------------+----------+------+------+------+------+------+\n",
      "| cpp_epochs_50_threads_8_neg_10 | spearman | 0.44 | 0.43 | 0.44 | 0.44 | 0.44 |\n",
      "| cpp_epochs_50_threads_8_neg_20 | spearman | 0.47 | 0.45 | 0.46 | 0.46 | 0.46 |\n",
      "| cpp_epochs_50_threads_1_neg_10 | spearman | 0.45 | 0.48 | 0.47 | 0.47 | 0.47 |\n",
      "+--------------------------------+----------+------+------+------+------+------+\n"
     ]
    }
   ],
   "source": [
    "display_results('Lexical Entailment (HyperLex)', entailment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Link Prediction for collaboration networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - quite tricky, since the loss function used for training the model on this network is different\n",
    "# Will require changes to how gradients are calculated in C++ code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
