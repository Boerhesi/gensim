{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Poincare Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how well poincare embeddings trained using this [implementation](https://github.com/TatsuyaShirakawa/poincare-embedding) perform on the tasks detailed in the [original paper](https://arxiv.org/pdf/1705.08039.pdf).\n",
    "\n",
    "This is the list of tasks - \n",
    "1. WordNet reconstruction\n",
    "2. WordNet link prediction\n",
    "3. Link prediction in collaboration networks\n",
    "4. Lexical entailment on HyperLex\n",
    "\n",
    "A more detailed explanation of the tasks and the evaluation methodology is present in the individual evaluation subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 C++ embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jayant/projects/gensim\n"
     ]
    }
   ],
   "source": [
    "% cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = '/home/jayant/projects/poincare-embedding/work'  # TODO: put model files into repo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from pygtrie import Trie\n",
    "from scipy.spatial.distance import euclidean, pdist\n",
    "from smart_open import smart_open\n",
    "\n",
    "def transform_cpp_embedding_to_kv(input_file, output_file, encoding='utf8'):\n",
    "    \"\"\"Given a C++ embedding tsv filepath, converts it to a KeyedVector-supported file\"\"\"\n",
    "    with smart_open(input_file, 'rb') as f:\n",
    "        lines = [line.decode(encoding) for line in f]\n",
    "    if not len(lines):\n",
    "         raise ValueError(\"file is empty\")\n",
    "    first_line = lines[0]\n",
    "    parts = first_line.rstrip().split(\"\\t\")\n",
    "    model_size = len(parts) - 1\n",
    "    vocab_size = len(lines)\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('%d %d\\n' % (vocab_size, model_size))\n",
    "        for line in lines:\n",
    "            f.write(line.replace('\\t', ' '))\n",
    "    \n",
    "        \n",
    "class PoincareEmbedding(object):\n",
    "    \"\"\"Load and perform distance operations on poincare embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, keyed_vectors):\n",
    "        \"\"\"Initialize PoincareEmbeddings via a KeyedVectors instance\"\"\"\n",
    "        self.kv = keyed_vectors\n",
    "        self.init_key_trie()\n",
    "        \n",
    "    def init_key_trie(self):\n",
    "        \"\"\"Setup trie containing vocab keys for quick prefix lookups\"\"\"\n",
    "        self.key_trie = Trie()\n",
    "        for key in self.kv.vocab:\n",
    "            self.key_trie[key] = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def poincare_dist(vector_1, vector_2):\n",
    "        \"\"\"Return poincare distance between two vectors\"\"\"\n",
    "        norm_1 = np.linalg.norm(vector_1)\n",
    "        norm_2 = np.linalg.norm(vector_2)\n",
    "        euclidean_dist = euclidean(vector_1, vector_2)\n",
    "        return np.arccosh(\n",
    "            1 + 2 * (\n",
    "                (euclidean_dist ** 2) / ((1 - norm_1 ** 2) * (1 - norm_2 ** 2))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    @classmethod\n",
    "    def load_poincare_cpp(cls, input_filename):\n",
    "        \"\"\"Load embeddings trained via C++ Poincare model\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to tsv file containing embeddings\n",
    "\n",
    "        Returns:\n",
    "            PoincareEmbedding instance\n",
    "\n",
    "        \"\"\"\n",
    "        keyed_vectors_filename = input_filename + '.kv'\n",
    "        transform_cpp_embedding_to_kv(input_filename, keyed_vectors_filename)\n",
    "        keyed_vectors = KeyedVectors.load_word2vec_format(keyed_vectors_filename)\n",
    "        os.unlink(keyed_vectors_filename)\n",
    "        return cls(keyed_vectors)\n",
    "    \n",
    "    def find_matching_keys(self, word):\n",
    "        \"\"\"Find all senses of given word in embedding vocabulary\"\"\"\n",
    "        matches = self.key_trie.items('%s.' % word)\n",
    "        matching_keys = [''.join(key_chars) for key_chars, value in matches]\n",
    "        return matching_keys\n",
    "\n",
    "    def get_vector(self, term):\n",
    "        \"\"\"Return vector for given term\"\"\"\n",
    "        return self.kv.word_vec(term)\n",
    "        \n",
    "    def get_all_distances(self, term):\n",
    "        \"\"\"Return distances to all terms for given term, including itself\"\"\"\n",
    "        term_vector = self.kv.word_vec(term)\n",
    "        all_vectors = self.kv.syn0\n",
    "        \n",
    "        euclidean_dists = np.linalg.norm(term_vector - all_vectors, axis=1)\n",
    "        norm = np.linalg.norm(term_vector)\n",
    "        all_norms = np.linalg.norm(all_vectors, axis=1)\n",
    "        return np.arccosh(\n",
    "            1 + 2 * (\n",
    "                (euclidean_dists ** 2) / ((1 - norm ** 2) * (1 - all_norms ** 2))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def get_distance(self, term_1, term_2):\n",
    "        \"\"\"Returns distance between vectors for input terms\n",
    "\n",
    "        Args:\n",
    "            term_1 (str)\n",
    "            term_2 (str)\n",
    "\n",
    "        Returns:\n",
    "            Poincare distance between the two terms (float)\n",
    "        \n",
    "        Note:\n",
    "            Raises KeyError if either term_1 or term_2 is absent from vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "        vector_1, vector_2 = self.kv[term_1], self.kv[term_2]\n",
    "        return self.poincare_dist(vector_1, vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "#     'wordnet_embeddings_2.tsv',\n",
    "#     'wordnet_embeddings_5.tsv',\n",
    "#     'wordnet_embeddings_10.tsv',\n",
    "    'wordnet_embeddings_20.tsv',\n",
    "    'wordnet_embeddings_20_ep50.tsv',\n",
    "    'wordnet_embeddings_50.tsv',\n",
    "#     'wordnet_embeddings_50_ep100.tsv',\n",
    "    'wordnet_embeddings_100.tsv',\n",
    "]\n",
    "embeddings = {fname: PoincareEmbedding.load_poincare_cpp(os.path.join(embeddings_dir, fname)) for fname in filenames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = embeddings['wordnet_embeddings_20_ep50.tsv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.11 s, sys: 0 ns, total: 7.11 s\n",
      "Wall time: 7.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, term in enumerate(test_embedding.kv.vocab.keys(), start=1):\n",
    "    if i > 1000:\n",
    "        break\n",
    "    dists = test_embedding.get_all_distances(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 WordNet reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "\n",
    "class ReconstructionEvaluation(object):\n",
    "    \"\"\"Evaluating reconstruction on given network for any embeddings\"\"\"\n",
    "    def __init__(self, filepath, embedding):\n",
    "        \"\"\"Initialize evaluation instance with tsv file containing relation pairs and embedding to be evaluated\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): path to tsv file containing relation pairs\n",
    "            embedding (PoincareEmbedding instance): embedding to be evaluated\n",
    "        \n",
    "        Returns\n",
    "            ReconstructionEvaluation instance\n",
    "\n",
    "        \"\"\"\n",
    "        items = set()\n",
    "        embedding_vocab = embedding.kv.vocab\n",
    "        positive_relations = defaultdict(set)\n",
    "        with smart_open(filepath, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                assert len(row) == 2, 'Hypernym pair has more than two items'\n",
    "                item_1_index = embedding_vocab[row[0]].index\n",
    "                item_2_index = embedding_vocab[row[1]].index\n",
    "                positive_relations[item_1_index].add(item_2_index)\n",
    "                items.update([item_1_index, item_2_index])\n",
    "        self.items = items\n",
    "        self.positive_relations = positive_relations\n",
    "        self.embedding = embedding\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_positive_item_ranks(distances, positive_item_indices):\n",
    "        \"\"\"Given a numpy array of distances and indices of positive items, compute ranks of positive item distances\n",
    "        \n",
    "        Args:\n",
    "            distances (numpy float array): np array of all distances for a specific item\n",
    "            positive_item_indices (list): list of indices of positive items\n",
    "        \n",
    "        Returns:\n",
    "            list of ranks of positive items in the same order as `positive_indices`\n",
    "        \"\"\"\n",
    "        positive_item_distances = distances[positive_item_indices]\n",
    "        negative_item_distances = np.ma.array(distances, mask=False)\n",
    "        negative_item_distances.mask[positive_item_indices] = True\n",
    "        # Compute how many negative item distances are less than each positive item distance, plus 1 for rank\n",
    "        ranks = (negative_item_distances < positive_item_distances[:, np.newaxis]).sum(axis=1) + 1\n",
    "        return list(ranks) \n",
    "\n",
    "    def evaluate_reconstruction(self, max_n=None):\n",
    "        \"\"\"Evaluate mean rank and MAP for reconstruction\n",
    "            \n",
    "        Args:\n",
    "            max_n (int or None): Maximum number of positive relations to evaluate, all if max_n is None\n",
    "        \n",
    "        Returns:\n",
    "            ??\n",
    "\n",
    "        \"\"\"\n",
    "        ranks = []\n",
    "        for i, item in enumerate(self.items, start=1):\n",
    "            if not i % 1000:\n",
    "                print('Evaluating item number %d: %s' % (i, item))\n",
    "            if item not in self.positive_relations:\n",
    "                continue\n",
    "            positive_items = list(self.positive_relations[item])\n",
    "            item_term = self.embedding.kv.index2word[item]\n",
    "            item_distances = self.embedding.get_all_distances(item_term)\n",
    "            positive_item_ranks = self.get_positive_item_ranks(item_distances, positive_items)\n",
    "            ranks += positive_item_ranks\n",
    "            if max_n is not None and i > max_n:\n",
    "                break\n",
    "        return np.mean(ranks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_instance = ReconstructionEvaluation(os.path.join(embeddings_dir, 'wordnet_noun_hypernyms.tsv'), test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating item number 1000: 999\n",
      "74.4394798266\n",
      "CPU times: user 9.34 s, sys: 0 ns, total: 9.34 s\n",
      "Wall time: 9.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(np.mean(eval_instance.evaluate_reconstruction(max_n=1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 WordNet link prediction\n",
    "TODO (tricky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 HyperLex lexical entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "class LexicalEntailmentEvaluation(object):\n",
    "    \"\"\"Evaluating reconstruction on given network for any embeddings\"\"\"\n",
    "    def __init__(self, filepath, embedding):\n",
    "        \"\"\"Initialize evaluation instance with HyperLex text file containing relation pairs\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): path to HyperLex text file\n",
    "            embedding (PoincareEmbedding instance): embedding to be evaluated\n",
    "        \n",
    "        Returns\n",
    "            LexicalEntailmentEvaluation instance\n",
    "\n",
    "        \"\"\"\n",
    "        expected_scores = {}\n",
    "        with smart_open(filepath, 'r') as f:\n",
    "            reader = csv.DictReader(f, delimiter=' ')\n",
    "            for row in reader:\n",
    "                word_1, word_2 = row['WORD1'], row['WORD2']\n",
    "                expected_scores[(word_1, word_2)] = float(row['AVG_SCORE'])\n",
    "        self.scores = expected_scores\n",
    "        self.embedding = embedding\n",
    "        self.alpha = 1000\n",
    "    \n",
    "    def score_function(self, word_1, word_2):\n",
    "        \"\"\"Given two terms, return the predicted score for them (extent to which term_1 is a type of term_2)\"\"\"\n",
    "        try:\n",
    "            word_1_terms = self.embedding.find_matching_keys(word_1)\n",
    "            word_2_terms = self.embedding.find_matching_keys(word_2)\n",
    "        except KeyError:\n",
    "            raise ValueError(\"No matching terms found for either %s or %s\" % (word_1, word_2))\n",
    "        min_distance = np.inf\n",
    "        min_term_1, min_term_2 = None, None\n",
    "        for term_1 in word_1_terms:\n",
    "            for term_2 in word_2_terms:\n",
    "                distance = self.embedding.get_distance(term_1, term_2)\n",
    "                if distance < min_distance:\n",
    "                    min_term_1, min_term_2 = term_1, term_2\n",
    "                    min_distance = distance\n",
    "        try:\n",
    "            assert min_term_1 is not None and min_term_2 is not None\n",
    "        except AssertionError:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        vector_1, vector_2 = self.embedding.get_vector(min_term_1), self.embedding.get_vector(min_term_2)\n",
    "        norm_1, norm_2 = np.linalg.norm(vector_1), np.linalg.norm(vector_2)\n",
    "        return -1 * (1 + self.alpha * (norm_2 - norm_1)) * distance\n",
    "        \n",
    "    def evaluate_spearman(self, embeddings):\n",
    "        \"\"\"Evaluate spearman scores for lexical entailment for given embeddings\n",
    "            \n",
    "        Args:\n",
    "            embeddings (PoincareEmbedding instance): embeddings for which evaluation is to be done\n",
    "        \n",
    "        Returns:\n",
    "            ??\n",
    "\n",
    "        \"\"\"\n",
    "        predicted_scores = []\n",
    "        expected_scores = []\n",
    "        skipped = 0\n",
    "        count = 0\n",
    "        for (word_1, word_2), expected_score in self.scores.items():\n",
    "            try:\n",
    "                predicted_score = self.score_function(word_1, word_2)\n",
    "            except ValueError:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            count += 1\n",
    "            predicted_scores.append(predicted_score)\n",
    "            expected_scores.append(expected_score)\n",
    "        print('Skipped pairs: %d out of %d' % (skipped, len(self.scores)))\n",
    "        spearman = spearmanr(expected_scores, predicted_scores)\n",
    "        return spearman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_instance = LexicalEntailmentEvaluation(os.path.join(embeddings_dir, 'nouns-verbs', 'hyperlex-nouns.txt'), test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped pairs: 182 out of 2163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.44053139956543591, pvalue=7.7820211828687839e-95)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_instance.evaluate_spearman(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Link Prediction for collaboration networks\n",
    "TODO (tricky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Link Prediction for collaboration networks\n",
    "TODO (tricky)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
